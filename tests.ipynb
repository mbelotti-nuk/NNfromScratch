{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(np.ndarray):\n",
    "    def __new__(cls, input_array, info=None):\n",
    "        # Input array is an already formed ndarray instance\n",
    "        # We first cast to be our class type\n",
    "        obj = np.asarray(input_array, dtype=np.float32).view(cls)\n",
    "        # add the new attribute to the created instance\n",
    "        obj.grad = np.zeros_like(input_array)\n",
    "        # Finally, we must return the newly created object:\n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        # see InfoArray.__array_finalize__ for comments\n",
    "        if obj is None: return\n",
    "        self.grad = getattr(obj, 'grad', None)\n",
    "\n",
    "class NeuralModule:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def backward(self, x, y):\n",
    "        pass\n",
    "    def update_params(self, lr):\n",
    "        pass\n",
    "    def zero_grads(self):\n",
    "        pass\n",
    "\n",
    "class NeuralModulesList:\n",
    "    modules : dict[int, NeuralModule]\n",
    "    def __init__(self,):\n",
    "        self.modules = dict()\n",
    "        self.n = 0\n",
    "        pass\n",
    "\n",
    "    def __iter__(self) -> Iterator[NeuralModule]:\n",
    "        return iter(self.modules.values())\n",
    "\n",
    "    def __next__(self) -> NeuralModule: \n",
    "        return iter(self.modules.values())\n",
    "\n",
    "    def __getitem__(self, index) -> NeuralModule:\n",
    "        return self.modules[index]\n",
    "\n",
    "    def append(self, module:NeuralModule):\n",
    "        self.modules[self.n] = module\n",
    "        self.n += 1\n",
    "\n",
    "    def extend(self, lst_modules:list[NeuralModule]):\n",
    "        for module in lst_modules:\n",
    "            self.modules[self.n] = module\n",
    "            self.n += 1        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer(NeuralModule):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        # layers\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.W = Tensor( np.random.randn(in_dim, out_dim) )\n",
    "        self.B = Tensor( np.zeros(out_dim) )\n",
    "        self.train = True\n",
    "        self.zero_grads()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        fan_in = self.W.shape[-1]\n",
    "        std = np.sqrt(2/fan_in)\n",
    "        self.W = Tensor( np.random.randn(*self.W.shape) * std )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.W + self.B\n",
    "    \n",
    "    def backward(self, x:Tensor, h:Tensor):\n",
    "        if self.train:\n",
    "            self.W.grad += x.T @ h.grad\n",
    "            self.B.grad += np.sum(h.grad, axis=0)\n",
    "            x.grad += h.grad @ self.W.T\n",
    "        return \n",
    "    \n",
    "    def zero_grads(self):\n",
    "        # derivatives\n",
    "        self.W.grad = np.zeros(shape=(self.in_dim, self.out_dim))\n",
    "        self.B.grad = np.zeros(shape=(self.out_dim))\n",
    "\n",
    "    def update_params(self, lr):\n",
    "        self.W = self.W - lr*self.W.grad\n",
    "        self.B = self.B - lr*self.B.grad\n",
    "    \n",
    "    def __matmul__(self, Y:np.array):\n",
    "        return Y @ self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[12.76087 , 12.266198, 12.111102, 13.734438, 13.307162,\n",
       "         13.426407, 12.06189 , 11.747089, 12.82989 , 13.590902],\n",
       "        [12.473997, 13.470976, 13.014597, 12.078024, 12.612325,\n",
       "         12.683188, 12.716412, 12.716234, 12.727184, 12.759898],\n",
       "        [12.702137, 12.517447, 12.75696 , 13.467454, 13.302441,\n",
       "         11.92293 , 12.729667, 13.514252, 12.757411, 12.621441],\n",
       "        [11.877745, 12.505779, 12.904266, 13.450518, 12.509647,\n",
       "         13.214955, 12.807601, 13.287572, 12.546663, 13.669604],\n",
       "        [12.956601, 12.944936, 12.455944, 13.808249, 13.227751,\n",
       "         12.823907, 13.032972, 12.653656, 13.48045 , 12.279826]],\n",
       "       dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((32,5))\n",
    "layer = NeuralLayer(5,10)\n",
    "\n",
    "layer.W + (layer.W**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(NeuralModule):\n",
    "    def __init__(self, p=0.8):\n",
    "        self.p = p\n",
    "        if self.p == 0:\n",
    "            self.p += 1e-6\n",
    "        if self.p == 1:\n",
    "            self.p -= 1e-6\n",
    "    \n",
    "    def __call__(self, x:Tensor):\n",
    "        self.mask = (np.random.rand(*x.shape) < self.p) / self.p \n",
    "        return x * self.mask\n",
    "    \n",
    "    def backward(self, x:Tensor, h:Tensor, weight_decay:float):\n",
    "        x.grad = h.grad * self.mask\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 1.25, 1.25, 1.25]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((1,5))\n",
    "dp = Dropout()\n",
    "dp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(NeuralModule):\n",
    "    def __init__(self, k_size, stride=(1,1), padding=(0,0)):\n",
    "        super().__init__()\n",
    "        self.k_size_h, self.k_size_w = k_size\n",
    "        self.stride = stride\n",
    "        self.pad_h, self.pad_w = padding\n",
    "\n",
    "    def __call__(self, x:Tensor) -> Tensor:\n",
    "        # pad_x = np.pad(x, pad_width=[(0,0),(0,0),(self.pad_h, self.pad_h),(self.pad_w, self.pad_w)], mode='constant', constant_values=(0))\n",
    "        # # 'strided_matrices' is the strided input image, shape = (batch, out_c, out_w, out_h, kx, ky)\n",
    "        # strided_matrices = self.get_strided_matrices(pad_x, self.k_size_h, self.k_size_w, self.stride)\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "        out_h = (h + 2* self.pad_h - self.k_size_h) // self.stride[0] + 1\n",
    "        out_w = (w + 2* self.pad_w - self.k_size_w) // self.stride[1] + 1\n",
    "\n",
    "        windows = self.get_windows(x, (n, c, out_h, out_w), self.k_size_h, self.pad_h, self.stride[0])\n",
    "\n",
    "        # Mean of every sub matrix, computed without considering the padd(np.nan)\n",
    "        out_x = Tensor( np.nanmean(windows, axis=(4, 5)) )\n",
    "\n",
    "        return out_x\n",
    "    \n",
    "    def get_windows(self,x:Tensor, output_size, kernel_size, padding=0, stride=1, dilate=0):\n",
    "        working_input = x\n",
    "        working_pad = padding\n",
    "        # dilate the input if necessary\n",
    "        if dilate != 0:\n",
    "            working_input = np.insert(working_input, range(1, x.shape[2]), 0, axis=2)\n",
    "            working_input = np.insert(working_input, range(1, x.shape[3]), 0, axis=3)\n",
    "\n",
    "        # pad the input if necessary\n",
    "        if working_pad != 0: \n",
    "            working_input = np.pad(working_input, pad_width=((0,), (0,), (working_pad,), (working_pad,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "        in_b, in_c, out_h, out_w = output_size\n",
    "        out_b, out_c, _, _ = x.shape\n",
    "        batch_str, channel_str, kern_h_str, kern_w_str = working_input.strides\n",
    "\n",
    "        return np.lib.stride_tricks.as_strided(\n",
    "            working_input,\n",
    "            (out_b, out_c, out_h, out_w, kernel_size, kernel_size),\n",
    "            (batch_str, channel_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str)\n",
    "        )   \n",
    "    \n",
    "    def get_strided_matrices(self, x:Tensor, k_h, k_w, s):\n",
    "        B, n_c, size_h, size_w = x.shape # B = batch, n_c = num. input channels, size_h = height, size_w = width\n",
    "        sh, sw = s\n",
    "        out_h = (size_h - k_h)//sh + 1\n",
    "        out_w = (size_w - k_w)//sw + 1\n",
    "        strides = (n_c*size_h*size_w, size_w*size_h, size_w*sh, sw, size_w, 1)\n",
    "        strides = tuple(i * x.itemsize for i in strides)\n",
    "        stride_matrices = np.lib.stride_tricks.as_strided(x, \n",
    "                                            shape=(B, n_c, out_h, out_w, k_h, k_w),\n",
    "                                            strides=strides)\n",
    "        return stride_matrices\n",
    "\n",
    "    def backward(self, x:Tensor, y:Tensor):\n",
    "        n, c, out_h, out_w = y.shape\n",
    "        _, _, in_h, in_w = x.shape\n",
    "        \n",
    "        windows = self.get_windows(y.grad, output_size=(n, c, in_h, in_w), kernel_size=self.k_size_h, padding=self.pad_h, dilate=self.stride[0])\n",
    "        print(f\"windows {windows.shape}   x {x.shape}\")\n",
    "        grad_per_pixel = 1 / (self.k_size_h * self.k_size_w)\n",
    "        x.grad += np.einsum( 'bchwkl->bchw', windows ) * grad_per_pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer(NeuralModule):\n",
    "    def __init__(self, in_ch, out_ch, k_size, stride=(1,1), padding=(0,0)):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "\n",
    "        self.k_size_h, self.k_size_w = k_size\n",
    "        self.stride = stride\n",
    "\n",
    "        n = out_ch * self.k_size_h * self.k_size_w\n",
    "        self.pad_h, self.pad_w = padding\n",
    "\n",
    "        self.train = True\n",
    "\n",
    "        # for each output channel, there is a 3D matrix of size  in_ch x k_size x k_size \n",
    "        self.K = Tensor( np.random.randn(out_ch, in_ch, self.k_size_h, self.k_size_w) * np.sqrt(2/n) )\n",
    "        self.B = Tensor( np.zeros( (1, out_ch, 1, 1)) )\n",
    "\n",
    "        # einstein summation indices for convolutions\n",
    "        # b : batch dimension\n",
    "        # o : output channels\n",
    "        # c : input channels\n",
    "        # i : kernel size on H\n",
    "        # j : kernel size on W\n",
    "        # h : size image on H\n",
    "        # w : size image on W\n",
    "        self.conv_modes = {'front':'bihwkl,oikl->bohw',  # 'bihwkl,oikl->bohw'    'ochw,bcijhw->boij'\n",
    "                           'back': 'fdkl,mcijkl->mdij',  \n",
    "                           'param':'bohw,bchwij->ocij' } \n",
    "\n",
    "\n",
    "    def convolution(self, x:Tensor, kernels:Tensor, s=(1,1), mode='front'):\n",
    "\n",
    "        stride_matrices = self.get_strided_matrices(x, self.k_size_h, self.k_size_w, s)\n",
    "        #print(f\"dilate x {x.shape}  kernel {kernels.shape} stride matrices {stride_matrices.shape}\")\n",
    "        return np.einsum(self.conv_modes[mode], kernels, stride_matrices)\n",
    "\n",
    "\n",
    "    def __call__(self, x:Tensor):\n",
    "        n, c, h, w = x.shape\n",
    "        out_h = (h - self.k_size_h + 2 * self.pad_h) // self.stride[0] + 1\n",
    "        out_w = (w - self.k_size_w + 2 * self.pad_w) // self.stride[1] + 1\n",
    "\n",
    "        windows = self.get_windows(x, (n, c, out_h, out_w), self.k_size_h, self.pad_h, self.stride[0])\n",
    "\n",
    "        out = np.einsum(self.conv_modes['front'], windows, self.K)\n",
    "\n",
    "        # add bias to kernels\n",
    "        out += self.B\n",
    "\n",
    "        self.cache = windows\n",
    "\n",
    "        return Tensor(out)\n",
    "\n",
    "    def backward(self, x:Tensor, h:Tensor):\n",
    "        if self.train:\n",
    "            windows = self.cache\n",
    "\n",
    "            padding = self.k_size_h - 1 if self.pad_h == 0 else self.pad_h\n",
    "\n",
    "            hgrad_windows = self.get_windows(h.grad, x.shape, self.k_size_h, padding=padding, stride=1, dilate=self.stride[0] - 1)\n",
    "            rot_kern = np.rot90(self.K, 2, axes=(2, 3))\n",
    "\n",
    "            self.B.grad += np.sum(h.grad, axis=(0, 2, 3)).reshape(self.B.grad.shape)\n",
    "            self.K.grad += np.einsum('bihwkl,bohw->oikl', windows, h.grad)\n",
    "            x.grad += np.einsum('bohwkl,oikl->bihw', hgrad_windows, rot_kern)\n",
    "\n",
    "        return \n",
    "\n",
    "\n",
    "    def get_windows(self,x:Tensor, output_size, kernel_size, padding=0, stride=1, dilate=0):\n",
    "        working_input = x\n",
    "        working_pad = padding\n",
    "        \n",
    "        # dilate the input if necessary\n",
    "        if dilate != 0:\n",
    "            working_input = np.insert(working_input, range(1, x.shape[2]), 0, axis=2)\n",
    "            working_input = np.insert(working_input, range(1, x.shape[3]), 0, axis=3)\n",
    "\n",
    "        # pad the input if necessary\n",
    "        if working_pad != 0: \n",
    "            working_input = np.pad(working_input, pad_width=((0,), (0,), (working_pad,), (working_pad,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "        in_b, in_c, out_h, out_w = output_size\n",
    "        out_b, out_c, _, _ = x.shape\n",
    "        batch_str, channel_str, kern_h_str, kern_w_str = working_input.strides\n",
    "\n",
    "        return np.lib.stride_tricks.as_strided(\n",
    "            working_input,\n",
    "            (out_b, out_c, out_h, out_w, kernel_size, kernel_size),\n",
    "            (batch_str, channel_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test forward pass conv layer and avg pool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward conv    | exact: False | approximate: False | maxdiff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t).item()\n",
    "  app = torch.allclose(dt, t)\n",
    "  maxdiff = (dt - t).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n",
    "\n",
    "torch_conv = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2)\n",
    "my_conv = ConvolutionalLayer(in_ch=3, out_ch=6, k_size=(5,5), padding=(2,2))\n",
    "\n",
    "k,b = torch_conv.parameters()\n",
    "my_conv.K = Tensor( k.detach().numpy() )\n",
    "my_conv.B = Tensor( b.detach().numpy().reshape(1,6,1,1) )\n",
    "\n",
    "\n",
    "x = Tensor(np.ones((32,3,28,28), dtype=np.float32))\n",
    "# forward pass\n",
    "my_y = my_conv(x)\n",
    "torch_y = torch_conv(torch.tensor(x))\n",
    "\n",
    "cmp('forward conv', torch.tensor(my_y), torch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward avg pool | exact: False | approximate: True  | maxdiff: 5.960464477539063e-08\n"
     ]
    }
   ],
   "source": [
    "my_avgpool = AvgPool(k_size=(2,2), stride=(2,2))\n",
    "torch_avg_pool = torch.nn.AvgPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "my_out_avg = my_avgpool(my_y)\n",
    "torch_out_avg = torch_avg_pool( torch.tensor(my_y) )\n",
    "\n",
    "cmp('forward avg pool', torch.tensor(my_out_avg), torch_out_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out 1 (32, 6, 14, 14)\n",
      "out 2 (32, 16, 10, 10)\n",
      "out 3 (32, 16, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "conv1 = ConvolutionalLayer(in_ch=3, out_ch=6, k_size=(5,5), padding=(2,2), stride=(2,2))\n",
    "conv2 = ConvolutionalLayer(in_ch=6, out_ch=16, k_size=(5,5), padding=(0,0), stride=(1,1))\n",
    "conv3 = ConvolutionalLayer(in_ch=16, out_ch=16, k_size=(5,5), padding=(2,2), stride=(2,2))\n",
    "x = Tensor(np.ones((32,3,28,28)))\n",
    "out_1 = conv1(x)\n",
    "print(f\"out 1 {out_1.shape}\")\n",
    "out_2 = conv2(out_1)\n",
    "print(f\"out 2 {out_2.shape}\")\n",
    "out_3 = conv3(out_2)\n",
    "print(f\"out 3 {out_3.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = ConvolutionalLayer(in_ch=6, out_ch=16, k_size=(5,5), padding=(0,0))\n",
    "avgpool = AvgPool(k_size=(2,2), stride=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out h (32, 16, 10, 10)\n",
      "out avg (32, 16, 5, 5)\n",
      "windows (32, 16, 10, 10, 2, 2)   x (32, 16, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.ones((32,6,14,14)))\n",
    "out_h = conv1(x)\n",
    "print(f\"out h {out_h.shape}\")\n",
    "out_avg = avgpool(out_h)\n",
    "print(f\"out avg {out_avg.shape}\")\n",
    "avgpool.backward(out_h, out_avg)\n",
    "conv1.backward(x,out_h)\n",
    "# 'param':'bohw,bcijhw->ocij'   '32 x 6 x 28 x 28'  '32 x 3 x 28 x 28 x 5 x 5' -> '6 x 3 x 5 x 5' \n",
    "# 'mfkl, mcijkl->fcij'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16, 10, 10) (32, 16, 10, 10)\n",
      "(32, 16, 5, 5) (32, 16, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "print(out_h.shape, out_h.grad.shape)\n",
    "print(out_avg.shape, out_avg.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(conv1.B.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16, 10, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_h.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16, 10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2 = ConvolutionalLayer(in_ch=6, out_ch=16, k_size=(5,5), padding=(0,0))\n",
    "conv2(np.ones((32,6,14,14))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(NeuralModule):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x:Tensor):\n",
    "        x[x<0] = 0\n",
    "        return x\n",
    "    def backward(self, x:Tensor, y:Tensor):\n",
    "        x.grad += y.grad * (x>0).astype(float)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(NeuralModule):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    def __call__(self, logits):\n",
    "        exp_logits = np.exp(logits - np.max(logits))  # Subtract max for stability\n",
    "        probabilities = exp_logits / exp_logits.sum(axis=-1, keepdims=True)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, logits:Tensor, target:Tensor)->Tensor:\n",
    "        exp_logits = np.exp(logits - np.max(logits))  # Subtract max for stability\n",
    "        probabilities = exp_logits / exp_logits.sum(axis=-1, keepdims=True)\n",
    "        real = np.clip(target, 1e-9, 1.0)  # Avoid log(0)\n",
    "        return  - np.sum(real * np.log(probabilities))/real.shape[0]\n",
    "    \n",
    "    def backward(self, logits, y):\n",
    "        exp_logits = np.exp(logits - np.max(logits))  # Subtract max for stability\n",
    "        probabilities = exp_logits / exp_logits.sum(axis=-1, keepdims=True)\n",
    "        #print(grad.shape, y.grad.shape)\n",
    "        logits.grad += probabilities-y\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self,loss):\n",
    "        self.loss = loss\n",
    "        self.layers = NeuralModulesList()\n",
    "        self.train = True\n",
    "    \n",
    "    def __call__(self, x:np.array):\n",
    "        if self.train:\n",
    "            self.intermediates = [Tensor(np.copy(x))]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if self.train: self.intermediates.append(Tensor(np.copy(x)))\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, y_hat:Tensor, y:Tensor):\n",
    "        loss_value = self.loss(y_hat, y)\n",
    "        if self.train: self.y = y\n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self):\n",
    "        self.y.grad = np.ones_like(self.y.grad)\n",
    "        self.loss.backward(self.intermediates[-1], self.y)\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            self.layers[i].backward(self.intermediates[i], self.intermediates[i+1])\n",
    "\n",
    "    def step(self, lr):\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(lr)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grads()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\n",
      "epoch 1\n",
      "\n",
      "epoch 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x = Tensor( np.random.randn(10, 4) )  # 100 samples, 4 features\n",
    "y_true = Tensor( np.eye(3)[np.random.choice(3, 10)] )  # One-hot labels (3 classes)\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "nn = ANN(loss)\n",
    "nn.layers.extend([\n",
    "    NeuralLayer(4, 5),\n",
    "    Relu(),\n",
    "    NeuralLayer(5, 3),\n",
    "]\n",
    ")\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"epoch {epoch}\")\n",
    "    print()\n",
    "    nn.zero_grads()\n",
    "    y_hat = nn(x)\n",
    "    loss = nn.get_loss(y_hat, y_true)\n",
    "    nn.backward()\n",
    "    nn.step(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing implementation on IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Access the features and target variable\n",
    "x = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
    "y = iris.target  # Target variable (species: 0 for setosa, 1 for versicolor, 2 for virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y = np.zeros((y.shape[0], len(np.unique(y))), dtype=np.float32)\n",
    "for i in range(y.shape[0]):\n",
    "    encoded_y[i, y[i]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "train_X, test_X, train_y, test_y = train_test_split(x, encoded_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        self.current_idx = 0  # Keeps track of batch index\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)  # Shuffle data at the start\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_idx = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_idx >= self.num_samples:\n",
    "            raise StopIteration  # Stop when all batches are processed\n",
    "\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[self.current_idx:self.current_idx + self.batch_size]\n",
    "        \n",
    "        # Slice data for the batch\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = self.y[batch_indices]\n",
    "\n",
    "        # Move to the next batch\n",
    "        self.current_idx += self.batch_size\n",
    "        \n",
    "        return Tensor(X_batch), Tensor(y_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "data_train = DataSet(train_X, train_y, batch_size=16)   \n",
    "data_test = DataSet(test_X, test_y, batch_size=16)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLoss()\n",
    "in_dim = train_X.shape[1]\n",
    "out_dim = train_y.shape[1]\n",
    "\n",
    "nn = ANN(loss)\n",
    "nn.layers.extend([\n",
    "    NeuralLayer(in_dim, 32),\n",
    "    Relu(),\n",
    "    NeuralLayer(32, 16),\n",
    "    Relu(),\n",
    "    NeuralLayer(16, out_dim),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, y):\n",
    "    sigma = Sigmoid()\n",
    "    probs = sigma(logits)\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "    reals = np.argmax(y, axis=-1)\n",
    "    correct = 0\n",
    "    for i in range(reals.shape[0]):\n",
    "        if reals[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 \t train: 1.000\t test: 0.740 \t accuracy 70.000%\n",
      "epoch  2 \t train: 0.713\t test: 0.572 \t accuracy 86.667%\n",
      "epoch  3 \t train: 0.609\t test: 0.490 \t accuracy 90.000%\n",
      "epoch  4 \t train: 0.523\t test: 0.423 \t accuracy 93.333%\n",
      "epoch  5 \t train: 0.469\t test: 0.356 \t accuracy 93.333%\n",
      "epoch  6 \t train: 0.423\t test: 0.321 \t accuracy 93.333%\n",
      "epoch  7 \t train: 0.390\t test: 0.295 \t accuracy 93.333%\n",
      "epoch  8 \t train: 0.361\t test: 0.274 \t accuracy 93.333%\n",
      "epoch  9 \t train: 0.336\t test: 0.250 \t accuracy 93.333%\n",
      "epoch 10 \t train: 0.318\t test: 0.235 \t accuracy 93.333%\n",
      "epoch 11 \t train: 0.300\t test: 0.225 \t accuracy 93.333%\n",
      "epoch 12 \t train: 0.293\t test: 0.203 \t accuracy 96.667%\n",
      "epoch 13 \t train: 0.292\t test: 0.195 \t accuracy 96.667%\n",
      "epoch 14 \t train: 0.261\t test: 0.183 \t accuracy 96.667%\n",
      "epoch 15 \t train: 0.262\t test: 0.183 \t accuracy 96.667%\n",
      "epoch 16 \t train: 0.257\t test: 0.172 \t accuracy 96.667%\n",
      "epoch 17 \t train: 0.232\t test: 0.167 \t accuracy 96.667%\n",
      "epoch 18 \t train: 0.235\t test: 0.167 \t accuracy 96.667%\n",
      "epoch 19 \t train: 0.233\t test: 0.158 \t accuracy 96.667%\n",
      "epoch 20 \t train: 0.220\t test: 0.155 \t accuracy 96.667%\n",
      "epoch 21 \t train: 0.211\t test: 0.150 \t accuracy 96.667%\n",
      "epoch 22 \t train: 0.208\t test: 0.140 \t accuracy 96.667%\n",
      "epoch 23 \t train: 0.204\t test: 0.139 \t accuracy 96.667%\n",
      "epoch 24 \t train: 0.191\t test: 0.136 \t accuracy 96.667%\n",
      "epoch 25 \t train: 0.192\t test: 0.131 \t accuracy 96.667%\n",
      "epoch 26 \t train: 0.185\t test: 0.130 \t accuracy 100.000%\n",
      "epoch 27 \t train: 0.176\t test: 0.123 \t accuracy 100.000%\n",
      "epoch 28 \t train: 0.169\t test: 0.122 \t accuracy 100.000%\n",
      "epoch 29 \t train: 0.171\t test: 0.120 \t accuracy 100.000%\n",
      "epoch 30 \t train: 0.160\t test: 0.121 \t accuracy 100.000%\n",
      "epoch 31 \t train: 0.173\t test: 0.119 \t accuracy 100.000%\n",
      "epoch 32 \t train: 0.160\t test: 0.114 \t accuracy 100.000%\n",
      "epoch 33 \t train: 0.165\t test: 0.114 \t accuracy 100.000%\n",
      "epoch 34 \t train: 0.160\t test: 0.107 \t accuracy 100.000%\n",
      "epoch 35 \t train: 0.157\t test: 0.107 \t accuracy 100.000%\n",
      "epoch 36 \t train: 0.146\t test: 0.102 \t accuracy 100.000%\n",
      "epoch 37 \t train: 0.145\t test: 0.104 \t accuracy 100.000%\n",
      "epoch 38 \t train: 0.149\t test: 0.103 \t accuracy 100.000%\n",
      "epoch 39 \t train: 0.151\t test: 0.101 \t accuracy 100.000%\n",
      "epoch 40 \t train: 0.143\t test: 0.098 \t accuracy 100.000%\n",
      "epoch 41 \t train: 0.143\t test: 0.093 \t accuracy 100.000%\n",
      "epoch 42 \t train: 0.135\t test: 0.099 \t accuracy 100.000%\n",
      "epoch 43 \t train: 0.133\t test: 0.096 \t accuracy 100.000%\n",
      "epoch 44 \t train: 0.131\t test: 0.098 \t accuracy 100.000%\n",
      "epoch 45 \t train: 0.131\t test: 0.090 \t accuracy 100.000%\n",
      "epoch 46 \t train: 0.123\t test: 0.086 \t accuracy 100.000%\n",
      "epoch 47 \t train: 0.131\t test: 0.087 \t accuracy 100.000%\n",
      "epoch 48 \t train: 0.122\t test: 0.085 \t accuracy 100.000%\n",
      "epoch 49 \t train: 0.126\t test: 0.085 \t accuracy 100.000%\n",
      "epoch 50 \t train: 0.118\t test: 0.084 \t accuracy 100.000%\n",
      "epoch 51 \t train: 0.121\t test: 0.081 \t accuracy 100.000%\n",
      "epoch 52 \t train: 0.114\t test: 0.082 \t accuracy 100.000%\n",
      "epoch 53 \t train: 0.113\t test: 0.078 \t accuracy 100.000%\n",
      "epoch 54 \t train: 0.109\t test: 0.077 \t accuracy 100.000%\n",
      "epoch 55 \t train: 0.109\t test: 0.075 \t accuracy 100.000%\n",
      "epoch 56 \t train: 0.109\t test: 0.077 \t accuracy 100.000%\n",
      "epoch 57 \t train: 0.110\t test: 0.078 \t accuracy 100.000%\n",
      "epoch 58 \t train: 0.111\t test: 0.081 \t accuracy 100.000%\n",
      "epoch 59 \t train: 0.104\t test: 0.077 \t accuracy 100.000%\n",
      "epoch 60 \t train: 0.114\t test: 0.072 \t accuracy 100.000%\n",
      "epoch 61 \t train: 0.103\t test: 0.073 \t accuracy 100.000%\n",
      "epoch 62 \t train: 0.109\t test: 0.072 \t accuracy 100.000%\n",
      "epoch 63 \t train: 0.104\t test: 0.071 \t accuracy 100.000%\n",
      "epoch 64 \t train: 0.100\t test: 0.069 \t accuracy 100.000%\n",
      "epoch 65 \t train: 0.100\t test: 0.071 \t accuracy 100.000%\n",
      "epoch 66 \t train: 0.103\t test: 0.069 \t accuracy 100.000%\n",
      "epoch 67 \t train: 0.098\t test: 0.069 \t accuracy 100.000%\n",
      "epoch 68 \t train: 0.100\t test: 0.065 \t accuracy 100.000%\n",
      "epoch 69 \t train: 0.098\t test: 0.066 \t accuracy 100.000%\n",
      "epoch 70 \t train: 0.092\t test: 0.067 \t accuracy 100.000%\n",
      "epoch 71 \t train: 0.094\t test: 0.066 \t accuracy 100.000%\n",
      "epoch 72 \t train: 0.093\t test: 0.064 \t accuracy 100.000%\n",
      "epoch 73 \t train: 0.100\t test: 0.067 \t accuracy 100.000%\n",
      "epoch 74 \t train: 0.100\t test: 0.065 \t accuracy 100.000%\n",
      "epoch 75 \t train: 0.091\t test: 0.062 \t accuracy 100.000%\n",
      "epoch 76 \t train: 0.088\t test: 0.062 \t accuracy 100.000%\n",
      "epoch 77 \t train: 0.088\t test: 0.060 \t accuracy 100.000%\n",
      "epoch 78 \t train: 0.101\t test: 0.060 \t accuracy 100.000%\n",
      "epoch 79 \t train: 0.098\t test: 0.059 \t accuracy 100.000%\n",
      "epoch 80 \t train: 0.084\t test: 0.060 \t accuracy 100.000%\n",
      "epoch 81 \t train: 0.086\t test: 0.057 \t accuracy 100.000%\n",
      "epoch 82 \t train: 0.084\t test: 0.057 \t accuracy 100.000%\n",
      "epoch 83 \t train: 0.087\t test: 0.056 \t accuracy 100.000%\n",
      "epoch 84 \t train: 0.088\t test: 0.060 \t accuracy 100.000%\n",
      "epoch 85 \t train: 0.082\t test: 0.055 \t accuracy 100.000%\n",
      "epoch 86 \t train: 0.088\t test: 0.057 \t accuracy 100.000%\n",
      "epoch 87 \t train: 0.082\t test: 0.054 \t accuracy 100.000%\n",
      "epoch 88 \t train: 0.086\t test: 0.055 \t accuracy 100.000%\n",
      "epoch 89 \t train: 0.080\t test: 0.054 \t accuracy 100.000%\n",
      "epoch 90 \t train: 0.080\t test: 0.054 \t accuracy 100.000%\n",
      "epoch 91 \t train: 0.092\t test: 0.052 \t accuracy 100.000%\n",
      "epoch 92 \t train: 0.080\t test: 0.055 \t accuracy 100.000%\n",
      "epoch 93 \t train: 0.085\t test: 0.052 \t accuracy 100.000%\n",
      "epoch 94 \t train: 0.078\t test: 0.053 \t accuracy 100.000%\n",
      "epoch 95 \t train: 0.077\t test: 0.054 \t accuracy 100.000%\n",
      "epoch 96 \t train: 0.081\t test: 0.054 \t accuracy 100.000%\n",
      "epoch 97 \t train: 0.080\t test: 0.055 \t accuracy 100.000%\n",
      "epoch 98 \t train: 0.074\t test: 0.051 \t accuracy 100.000%\n",
      "epoch 99 \t train: 0.081\t test: 0.052 \t accuracy 100.000%\n",
      "epoch 100 \t train: 0.080\t test: 0.051 \t accuracy 100.000%\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, test_accuracies = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, test_loss, test_accuracy = 0,0,0\n",
    "    # train\n",
    "    nn.train = True\n",
    "    i = 0\n",
    "    for batch in data_train:\n",
    "        i += 1\n",
    "        nn.zero_grads()\n",
    "        x, y = batch\n",
    "        y_hat = nn(x)\n",
    "        train_loss += nn.get_loss(y_hat, y)\n",
    "        nn.backward()\n",
    "        nn.step(lr)\n",
    "    train_losses.append(train_loss/i)\n",
    "    \n",
    "    nn.train = False\n",
    "    i = 0\n",
    "    for batch in data_test:\n",
    "        i += 1\n",
    "        x, y = batch\n",
    "        y_hat = nn(x)\n",
    "        test_accuracy += accuracy(y_hat, y)\n",
    "        test_loss += nn.get_loss(y_hat, y)\n",
    "    test_losses.append(test_loss/i)\n",
    "    test_accuracies.append( test_accuracy/data_test.X.shape[0] )\n",
    "\n",
    "    print(f\"epoch {epoch+1:2.0f} \\t train: {train_losses[-1]:3.3f}\\t test: {test_losses[-1]:3.3f} \\t accuracy {100*test_accuracies[-1]:3.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self,loss):\n",
    "        self.loss = loss\n",
    "        self.layers = NeuralModulesList()\n",
    "        self.train = True\n",
    "    \n",
    "    def __call__(self, x:np.array):\n",
    "        if self.train:\n",
    "            self.intermediates = [Tensor(np.copy(x))]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if self.train: self.intermediates.append(Tensor(np.copy(x)))\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self, y_hat:Tensor, y:Tensor):\n",
    "        loss_value = self.loss(y_hat, y)\n",
    "        if self.train: self.y = y\n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self):\n",
    "        self.y.grad = np.ones_like(self.y.grad)\n",
    "        self.loss.backward(self.intermediates[-1], self.y)\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            print(f\"x {self.intermediates[i].grad.shape}  y {self.intermediates[i+1].grad.shape}\")\n",
    "            self.layers[i].backward(self.intermediates[i], self.intermediates[i+1])\n",
    "            print(f\"mean {np.mean(x.grad)}\")\n",
    "\n",
    "    def step(self, lr):\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(lr)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grads()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flatten(NeuralModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def __call__(self, x:Tensor):\n",
    "        self.x_shape = x.shape\n",
    "        return x.reshape(self.x_shape[0],-1)\n",
    "    \n",
    "    def backward(self, x:Tensor, y:Tensor, weight_decay=0):\n",
    "        x.grad += y.grad.reshape(self.x_shape)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ANN(loss,)\n",
    "\n",
    "conv = ConvolutionalLayer(in_ch=1, out_ch=3, k_size=(3,3), padding=(1,1))\n",
    "pool = AvgPool(k_size=(3,3), padding=(0,0))\n",
    "\n",
    "nn.layers.extend([\n",
    "    ConvolutionalLayer(in_ch=1, out_ch=3, k_size=(3,3), padding=(1,1)),\n",
    "    Relu(),\n",
    "    AvgPool(k_size=(3,3), padding=(0,0)),\n",
    "    Flatten(),\n",
    "    NeuralLayer(in_dim=12, out_dim=1),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (32, 12)  y (32, 1)\n",
      "mean 0.0\n",
      "x (32, 3, 2, 2)  y (32, 12)\n",
      "mean 0.0\n",
      "x (32, 3, 4, 4)  y (32, 3, 2, 2)\n",
      "windows (32, 3, 4, 4, 3, 3)   x (32, 3, 4, 4)\n",
      "mean 0.0\n",
      "x (32, 3, 4, 4)  y (32, 3, 4, 4)\n",
      "mean 0.0\n",
      "x (32, 1, 4, 4)  y (32, 3, 4, 4)\n",
      "mean 0.0\n"
     ]
    }
   ],
   "source": [
    "x = Tensor( np.ones((32,1,4,4)) )\n",
    "y = Tensor( np.ones((32,1)) )\n",
    "y_hat = nn(x)\n",
    "train_loss = nn.get_loss(y_hat, y)\n",
    "nn.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(-0.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
